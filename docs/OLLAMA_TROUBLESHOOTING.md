# Guide de d√©pannage Ollama

Ce guide vous aide √† r√©soudre les probl√®mes courants avec l'int√©gration Ollama.

## üî¥ Erreur 404 Not Found

### Sympt√¥me

```json
{
  "status": "error",
  "errors": [
    "Tentative 1/3: √âchec de la g√©n√©ration apr√®s 3 tentatives: Ollama API error: 404 Not Found",
    "Tentative 2/3: √âchec de la g√©n√©ration apr√®s 3 tentatives: Ollama API error: 404 Not Found",
    "Tentative 3/3: √âchec de la g√©n√©ration apr√®s 3 tentatives: Ollama API error: 404 Not Found"
  ]
}
```

### Cause

L'erreur 404 signifie que le mod√®le sp√©cifi√© dans `OLLAMA_MODEL` **n'existe pas** dans votre instance Ollama.

### Solution rapide

#### 1. Lancer le script de diagnostic

```bash
bash scripts/diagnose-ollama-model.sh
```

Ce script va :
- ‚úÖ V√©rifier que le container Ollama est actif
- ‚úÖ Lister tous les mod√®les disponibles
- ‚úÖ Comparer avec votre configuration
- ‚úÖ Sugg√©rer la correction exacte

#### 2. Diagnostic manuel

Si vous pr√©f√©rez diagnostiquer manuellement :

**a) Lister les mod√®les disponibles dans Ollama :**

```bash
# Trouver le nom du container
docker ps | grep ollama

# Lister les mod√®les (remplacez <container-name> par le nom de votre container)
docker exec <container-name> ollama list
```

**b) V√©rifier votre configuration :**

```bash
# Voir la valeur actuelle
grep OLLAMA_MODEL .env
```

**c) Comparer les deux noms**

Le nom doit correspondre **EXACTEMENT** (majuscules/minuscules, tirets, etc.)

### Solutions possibles

#### Solution 1 : Corriger le nom du mod√®le

Si le mod√®le existe mais le nom ne correspond pas exactement :

```bash
# Dans .env, remplacez par le nom exact (exemple)
OLLAMA_MODEL=openllm-france/lucie-7b-instruct
```

**Exemples courants de noms :**
- ‚ùå `OpenLLM-France/Lucie-7B-Instruct:latest`
- ‚úÖ `openllm-france/lucie-7b-instruct:latest`

ou

- ‚ùå `qwen:latest`
- ‚úÖ `qwen2.5:latest`

#### Solution 2 : T√©l√©charger le mod√®le manquant

Si le mod√®le n'existe pas du tout dans Ollama :

```bash
# Pull le mod√®le dans le container
docker exec <container-name> ollama pull <model-name>

# Exemple pour Lucie
docker exec easycook-ollama ollama pull openllm-france/lucie-7b-instruct

# Exemple pour Qwen
docker exec easycook-ollama ollama pull qwen2.5
```

**‚è±Ô∏è Attention :** Le t√©l√©chargement peut prendre plusieurs minutes selon la taille du mod√®le.

#### Solution 3 : Utiliser un mod√®le alternatif

Si vous voulez simplement que √ßa fonctionne rapidement :

```bash
# T√©l√©charger un mod√®le l√©ger et rapide
docker exec easycook-ollama ollama pull mistral

# Puis configurer dans .env
OLLAMA_MODEL=mistral
```

Mod√®les recommand√©s :
- `mistral` - L√©ger, performant (7B param√®tres)
- `qwen2.5` - Tr√®s bon pour le fran√ßais
- `llama3.2` - Derni√®re version de Meta

### V√©rification finale

Une fois corrig√©, testez avec le health check :

```bash
# Via curl
curl http://localhost:3000/api/health/ollama

# Ou dans votre navigateur
http://localhost:3000/api/health/ollama
```

R√©ponse attendue :

```json
{
  "status": "healthy",
  "healthy": true,
  "message": "‚úÖ Ollama is healthy and ready with model \"your-model\""
}
```

## üî¥ Timeout apr√®s X secondes

### Sympt√¥me

```
Timeout apr√®s 180 secondes. Le mod√®le met peut-√™tre du temps √† se charger (premi√®re requ√™te).
```

### Cause

Le mod√®le n'est pas encore charg√© en m√©moire. **La premi√®re requ√™te peut prendre 60-120 secondes.**

### Solution

1. **Patienter** - La premi√®re g√©n√©ration peut √™tre longue
2. **Augmenter le timeout** dans `.env` :

```bash
OLLAMA_TIMEOUT=300000  # 5 minutes
```

3. **Pr√©-charger le mod√®le** au d√©marrage du container :

```bash
# G√©n√©rer une requ√™te de test
docker exec easycook-ollama ollama run <model-name> "Hello"
```

## üî¥ Service non disponible

### Sympt√¥me

```
‚ùå Service d'IA Ollama non disponible
```

### Solutions

#### 1. V√©rifier que le container est actif

```bash
docker ps | grep ollama
```

Si absent, d√©marrer :

```bash
docker-compose up -d ollama
```

#### 2. V√©rifier le port

```bash
docker port <container-name>
# Devrait afficher : 11434/tcp -> 0.0.0.0:11434
```

#### 3. Tester la connectivit√©

```bash
# Depuis votre machine h√¥te
curl http://localhost:11434/api/tags

# Si √ßa ne fonctionne pas, v√©rifier OLLAMA_BASE_URL dans .env
```

#### 4. V√©rifier OLLAMA_BASE_URL

Dans `.env` :
- **En local** : `OLLAMA_BASE_URL=http://localhost:11434`
- **Dans Docker Compose** : `OLLAMA_BASE_URL=http://ollama:11434`

## üõ†Ô∏è Commandes utiles

### Gestion des mod√®les

```bash
# Lister les mod√®les
docker exec <container> ollama list

# Pull un mod√®le
docker exec <container> ollama pull <model>

# Supprimer un mod√®le
docker exec <container> ollama rm <model>

# Renommer/copier un mod√®le
docker exec <container> ollama cp <source> <destination>
```

### Logs et debugging

```bash
# Voir les logs du container
docker logs <container-name> -f

# Voir les logs de l'application
npm run dev
# Regarder les lignes commen√ßant par [Ollama]
```

### Tests

```bash
# Test de g√©n√©ration simple
docker exec <container> ollama run <model> "Bonjour, √©cris une recette courte"

# Health check API
curl http://localhost:3000/api/health/ollama
```

## üìã Checklist compl√®te

Si rien ne fonctionne, v√©rifiez dans l'ordre :

- [ ] Container Ollama est running : `docker ps | grep ollama`
- [ ] Port 11434 est expos√© : `docker port <container>`
- [ ] Service r√©pond : `curl http://localhost:11434/api/tags`
- [ ] Mod√®le existe : `docker exec <container> ollama list`
- [ ] OLLAMA_MODEL dans .env correspond au nom exact
- [ ] OLLAMA_BASE_URL est correct (localhost:11434 ou ollama:11434)
- [ ] Health check OK : `curl http://localhost:3000/api/health/ollama`

## üÜò Besoin d'aide ?

Si le probl√®me persiste :

1. **Lancez le diagnostic** : `bash scripts/diagnose-ollama-model.sh`
2. **V√©rifiez les logs** : `docker logs <container> | tail -100`
3. **Cr√©ez une issue** avec les logs et la sortie du diagnostic

## üìö Ressources

- [Documentation Ollama](https://github.com/ollama/ollama)
- [Liste des mod√®les disponibles](https://ollama.com/library)
- [Docker Compose Ollama](https://hub.docker.com/r/ollama/ollama)
